{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/work/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "import json\n",
    "\n",
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models import shared\n",
    "from unstructured_client.models.errors import SDKError\n",
    "\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import dict_to_elements, elements_to_json\n",
    "from unstructured.partition.text import partition_text\n",
    "from unstructured.partition.docx import partition_docx\n",
    "from unstructured.partition.doc import partition_doc\n",
    "from unstructured.partition.image import partition_image\n",
    "\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "from unstructured.chunking.title import chunk_by_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_api_key=\"\"\n",
    "qdrant_api_url=\"https://113d73aa-ce94-4871-8c26-bd7208c08a88.europe-west3-0.gcp.cloud.qdrant.io\"\n",
    "\n",
    "client = QdrantClient(\n",
    "    qdrant_api_url,\n",
    "    api_key=qdrant_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Docs using unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPartition(file_path, file_type):\n",
    "    extension = os.path.splitext(file_path)[-1].lower()\n",
    "    file_elements = createPartitionByFileType(file_path, extension)\n",
    "    file_dict = [el.to_dict() for el in file_elements]\n",
    "    return file_dict\n",
    " \n",
    "        \n",
    "def createPartitionByFileType(file_path, file_type):\n",
    "    match file_type:\n",
    "        case '.pdf':\n",
    "            return partition_pdf(file_path)\n",
    "        case '.doc':\n",
    "            return partition_doc(file_path)\n",
    "        case '.docx':\n",
    "            return partition_docx(file_path)\n",
    "        case '.jpeg' | '.png' | '.jpeg':\n",
    "            return createImagePartition(file_path, False)\n",
    "        case '.txt':\n",
    "            return partition_text(file_path)\n",
    "        case _:\n",
    "            return partition(file_path)\n",
    "\n",
    "def createImagePartition(file_path, isOcrEnabled):\n",
    "    if isOcrEnabled:\n",
    "        return partition_image(file_path, strategy=\"ocr_only\")\n",
    "    else:\n",
    "        return  partition_image(file_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizePDF(document_dict, partition_labels, chapter):\n",
    "    label_ids = {}\n",
    "    for element in document_dict:\n",
    "        for label in partition_labels:\n",
    "            if element[\"text\"] == label and element[\"type\"] == \"Title\":\n",
    "                label_ids[element[\"element_id\"]] = chapter\n",
    "                break\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunk(file_element):\n",
    "    chunks = chunk_by_title(\n",
    "    file_element,\n",
    "    combine_text_under_n_chars=100,\n",
    "    max_characters=3000,\n",
    "    )   \n",
    "    return chunks\n",
    "\n",
    "# Add metadata to chunks\n",
    "def processChunk(chunks):\n",
    "    documents = []\n",
    "    for element in chunks:\n",
    "        metadata = element.metadata.to_dict()\n",
    "        print(metadata)\n",
    "        del metadata[\"languages\"]\n",
    "        metadata[\"source\"] = metadata[\"filename\"]\n",
    "        metadata[\"hash_id\"] = createHash(element.id)\n",
    "        documents.append(Document(page_content=element.text, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "def createHash(id):\n",
    "   return hashlib.sha256().update((id).encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedding_models = {\n",
    "    \"small\": \"text-embedding-3-small\",\n",
    "    \"ada\": \"text-embedding-ada-002\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createQdrantEmbeddings(documents, collection_name):\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def folderReader(folder_path):\n",
    "#     return SimpleDirectoryReader(folder_path).load_data()\n",
    "\n",
    "# def buildQdrantVectoreStore(documents):\n",
    "#     vector_store = QdrantVectorStore(client=client, collection_name=\"paul_graham\")\n",
    "#     storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#     index = VectorStoreIndex.from_documents(\n",
    "#         documents,\n",
    "#         storage_context=storage_context,\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
